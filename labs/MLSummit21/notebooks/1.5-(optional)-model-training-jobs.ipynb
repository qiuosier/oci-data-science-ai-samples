{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Oracle Cloud Infrastructure Data Science Demo Notebook\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.<br>\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font> Using Data Science Jobs to Automate Model Building and Training</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> OCI Data Science PM Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>NOTE: This notebook was run in the TensorFlow 2.7 for CPU (slug: `tensorflow27_p37_cpu_v1`) conda environment with ADS version 2.5.10. Upgrade your version of ADS (see cell below) and restart your kernel.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install oracle-ads==2.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "print(ads.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Simple Model training Script \n",
    "\n",
    "First step will be to create a simple model training script that is identical to the training steps we wrote in notebook `1-model-training.ipynb`. We store the training script inside of a job artifact folder (`./job-artifact`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the script that will be executed as a Data Science Job. \n",
    "\n",
    "The script: \n",
    "* pulls the data from object storage;\n",
    "* does data transformation on the data; \n",
    "* creates an sklearn pipeline object; \n",
    "* trains a random forest classifier; \n",
    "* saves the sklearn pipeline object (joblib) to disk in the model artifact folder; \n",
    "* uses the model artifact files in the model-artifact folder to create a model artifact object from a local folder (ModelArtifact(path))\n",
    "* saves the model to the model catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job-artifact/attrition-job.py \n",
    "\n",
    "import io\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "from os import path \n",
    "from os.path import expanduser\n",
    "from os.path import join\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "from ads.common.model import ADSModel\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "from dataframelabelencoder import DataFrameLabelEncoder\n",
    "\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import joblib \n",
    "\n",
    "import ads \n",
    "ads.set_auth(\"resource_principal\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "# downloading the data from object storage: \n",
    "bucket_name = \"hosted-ds-datasets\"\n",
    "namespace = \"bigdatadatasciencelarge\"\n",
    "ds = DatasetFactory.open(\n",
    "        \"oci://{}@{}/synthetic/orcl_attrition.csv\".format(bucket_name, namespace), \n",
    "    target=\"Attrition\",  storage_options={'config':{}, 'region': 'us-ashburn-1', 'tenancy':os.environ['TENANCY_OCID']}).set_positive_class('Yes')\n",
    "\n",
    "print(\"done downloading data\")\n",
    "\n",
    "# Transforming the data: \n",
    "transformed_ds = ds.auto_transform(fix_imbalance=False)\n",
    "train, test = transformed_ds.train_test_split()\n",
    "\n",
    "print(\"done auto-transform data\")\n",
    "\n",
    "X = train.X.copy()\n",
    "y = train.y.copy()\n",
    "\n",
    "le = DataFrameLabelEncoder()\n",
    "X = le.fit_transform(X)\n",
    "\n",
    "# Training the Random Forest Classifier: \n",
    "sk_clf = RandomForestClassifier(random_state=42)\n",
    "sk_clf.fit(X, y)\n",
    "\n",
    "sk_model = make_pipeline(le, sk_clf)\n",
    "\n",
    "print(\"completed model training\")\n",
    "\n",
    "\n",
    "# Path to artifact directory for my sklearn model: \n",
    "path = \"model-artifact/\"\n",
    "\n",
    "\n",
    "print(\"serializing sklearn object\")\n",
    "print(f\"current path:  {os.path.abspath('.')}\")\n",
    "print(f\"list content of cwd: {os.listdir('.')}\")\n",
    "print(f\"model-artifact exists:  {os.path.exists('./model-artifact/')}\")\n",
    "print(f\"full path exists: {os.path.exists(os.path.join(os.path.abspath('.'), path))}\")\n",
    "\n",
    "# Creating a joblib pickle object of my random forest model: \n",
    "#joblib.dump(sk_model, os.path.join(os.path.abspath(\".\"), path, \"model.joblib\"))\n",
    "joblib.dump(sk_model, \"/home/datascience/decompressed_artifact/job-artifact/model-artifact/model.joblib\")\n",
    "\n",
    "print(\"preparing model artifact\")\n",
    "\n",
    "#sk_artifact = ModelArtifact(os.path.join(os.path.abspath(\".\"), path))\n",
    "sk_artifact = ModelArtifact(\"/home/datascience/decompressed_artifact/job-artifact/model-artifact/\")\n",
    "print(\"done creating sk_artifact\")\n",
    "\n",
    "\n",
    "sk_artifact.populate_schema(X_sample=train.X, y_sample=train.y)\n",
    "\n",
    "print(\"done populating schema\")\n",
    "\n",
    "print(\"done preparing model artifact\")\n",
    "\n",
    "print(\"saving model artifact to catalog\")\n",
    "\n",
    "# Save the model to the catalog: \n",
    "mc_model = sk_artifact.save(project_id=os.environ['PROJECT_OCID'],\n",
    "                            compartment_id=os.environ['JOB_RUN_COMPARTMENT_OCID'],\n",
    "                            training_id=os.environ['JOB_RUN_OCID'],\n",
    "                            display_name=\"sklearn-employee-attrition-from-job\",\n",
    "                            description=\"simple sklearn model to predict employee attrition\", \n",
    "                            ignore_pending_changes=True)\n",
    "\n",
    "print(\"done saving model artifact to catalog\")\n",
    "\n",
    "print(mc_model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Science Job and Job Run\n",
    "\n",
    "Here are we creating a [Data Science Job and a Job Run](https://docs.oracle.com/en-us/iaas/data-science/using/jobs-about.htm) using the ADS library. \n",
    "\n",
    "Data Science jobs enable custom tasks because you can apply any use case you have, such as data preparation, model training, hyperparameter tuning, batch inference, and so on.\n",
    "\n",
    "Using jobs, you can:\n",
    "\n",
    "* Run machine learning (ML) or data science tasks outside of your notebook sessions in JupyterLab.\n",
    "* Operationalize discrete data science and machine learning tasks as reusable runnable operations.\n",
    "* Automate your typical MLOps or CI/CD pipeline.\n",
    "* Execute batches or workloads triggered by events or actions.\n",
    "* Batch, mini batch, or distributed batch job inference.\n",
    "\n",
    "Jobs are run in virtual machines (VMs) in the OCI Data Science service tenancy. The VM will be running for the duration of your job and will shut itself down at the completion of your job script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.common.oci_logging import OCILogGroup, OCILog\n",
    "import ads\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "# here we are using resource principal to authenticate with the Data Science Jobs API: \n",
    "ads.set_auth(auth=\"resource_principal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the [log group OCID and log OCID](https://docs.oracle.com/en-us/iaas/Content/Logging/Task/managinglogs.htm) that you want to attach to your Job. You need to create the log group and log through the OCI Logging service first and grant Job Run access to the Logging service on your behalf. This is done through [resource principals for Job Runs and an IAM policy](https://docs.oracle.com/en-us/iaas/data-science/using/policies.htm#policy-examples). \n",
    "\n",
    "Specifying logs is optional but highly recommended. Without logging enabled, it is very difficult to troubleshoot job runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_group_id = \"\"\n",
    "log_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.jobs import Job\n",
    "from ads.jobs import DataScienceJob, ScriptRuntime\n",
    "\n",
    "job_name = 'attrition-model-training-job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = Job(job_name).with_infrastructure(DataScienceJob().\\\n",
    "                                with_shape_name(\"VM.Standard2.1\").\\\n",
    "                                with_log_id(log_id).\\\n",
    "                                with_log_group_id(log_group_id)).\\\n",
    "            with_runtime(ScriptRuntime().\\\n",
    "                         with_source(\"job-artifact\", entrypoint=\"job-artifact/attrition-job.py\").\\\n",
    "                         with_service_conda(\"tensorflow27_p37_cpu_v1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a job. The job itself will not trigger the execution of the job script. Think of the job as the resource that contains the configuration and definition of the task to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsjob = job.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are executing the job by creating a job run. The `watch()` method will only be available if you have enabled logs for your job or job run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsjob.run().watch()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow27_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow27_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
