{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Tensorflow training using Horovod via OCI Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Train](#Train)\n",
    "1. [Setup IAM](#Setup%20IAM)\n",
    "1. [Build](#Build)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and MXNet. This notebook example shows how to use Horovod with Tensorflow in OCI Data Science Jobs . OCI Data Science currently support Elastic Horovod workloads with gloo backend.\n",
    "\n",
    "For more information about the Horovod with TensorFlow , please visit [Horovod-Tensorflow](https://horovod.readthedocs.io/en/stable/tensorflow.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install ads package >= 2.5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install docker:\n",
    "\n",
    "https://docs.docker.com/get-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Set IAM Policies\n",
    "\n",
    "Following Policies need to be in place in the OCI IAM service. This would allow OCI datascience job runs to access needed services such as logging, object storage, vcns etc.\n",
    "\n",
    "#### Create the Dynamic Group\n",
    "```\n",
    "ALL {resource.type = ‘datasciencejobrun’, resource.compartment.id = <COMPARTMENT_OCID>}\n",
    "```\n",
    "\n",
    "#### Create policies\n",
    "```\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to use log-content in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to use log-groups in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to inspect repos in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to inspect vcns in compartment <COMPARTMENT_NAME>\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to manage objects in compartment <COMPARTMENT_NAME> where any {target.bucket.name='<BUCKET_NAME>'}\n",
    "Allow dynamic-group <DYNAMIC_GROUP_NAME> to manage buckets in compartment <COMPARTMENT_NAME> where any {target.bucket.name='<BUCKET_NAME>'}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Create VCN and private subnet\n",
    "\n",
    "Horovod Distributed Training requires nodes to communicate to each other. Therefor, network settings need to be provisioned. Create a VCN and a private subnet. Ths subnet id of this private subnet needs to be configured in the workload yaml file ('train.yaml')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainining Script\n",
    "\n",
    "This section will create a horovod tensorflow training script. This is the training code that executes on the horovod cluster. The script must confirm to Elastic Horovod apis.\n",
    "\n",
    "The following script uses Horovod framework for distributed training where Horovod related apis are commented starting with `Horovod:`. <br> For example, `Horovod: add Horovod DistributedOptimizer`, `Horovod: initialize optimize`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "# Script adapted from https://github.com/horovod/horovod/blob/master/examples/elastic/tensorflow2/tensorflow2_keras_mnist_elastic.py\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tensorflow 2.0 Keras MNIST Example')\n",
    "\n",
    "parser.add_argument('--use-mixed-precision', action='store_true', default=False,\n",
    "                    help='use mixed precision for training')\n",
    "\n",
    "parser.add_argument('--data-dir',\n",
    "                    help='location of the training dataset in the local filesystem (will be downloaded if needed)',\n",
    "                    default='/code/data/mnist.npz')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.use_mixed_precision:\n",
    "    print(f\"using mixed precision {args.use_mixed_precision}\")\n",
    "    if LooseVersion(tf.__version__) >= LooseVersion('2.4.0'):\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "    else:\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "\n",
    "import numpy as np\n",
    "minist_local = args.data_dir\n",
    "\n",
    "def load_data():\n",
    "    print(\"using pre-fetched dataset\")\n",
    "    with np.load(minist_local, allow_pickle=True) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(mnist_images, mnist_labels), _ = load_data() if os.path.exists(minist_local) else tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\n",
    "             tf.cast(mnist_labels, tf.int64))\n",
    ")\n",
    "dataset = dataset.repeat().shuffle(10000).batch(128)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "scaled_lr = 0.001 * hvd.size()\n",
    "opt = tf.optimizers.Adam(scaled_lr)\n",
    "\n",
    "# Horovod: add Horovod DistributedOptimizer.\n",
    "opt = hvd.DistributedOptimizer(\n",
    "    opt, backward_passes_per_step=1, average_aggregated_gradients=True)\n",
    "\n",
    "# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\n",
    "# uses hvd.DistributedOptimizer() to compute gradients.\n",
    "model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "                    optimizer=opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    experimental_run_tf_function=False)\n",
    "\n",
    "# Horovod: initialize optimizer state so we can synchronize across workers\n",
    "# Keras has empty optimizer variables() for TF2:\n",
    "# https://sourcegraph.com/github.com/tensorflow/tensorflow@v2.4.1/-/blob/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L351:10\n",
    "model.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n",
    "\n",
    "state = hvd.elastic.KerasState(model, batch=0, epoch=0)\n",
    "\n",
    "def on_state_reset():\n",
    "    tf.keras.backend.set_value(state.model.optimizer.lr,  0.001 * hvd.size())\n",
    "    # Re-initialize, to join with possible new ranks\n",
    "    state.model.fit(dataset, steps_per_epoch=1, epochs=1, callbacks=None)\n",
    "\n",
    "state.register_reset_callbacks([on_state_reset])\n",
    "\n",
    "callbacks = [\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "    hvd.elastic.UpdateEpochStateCallback(state),\n",
    "    hvd.elastic.UpdateBatchStateCallback(state),\n",
    "    hvd.elastic.CommitStateCallback(state),\n",
    "]\n",
    "\n",
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "# save the artifacts in the OCI__SYNC_DIR dir.\n",
    "artifacts_dir=os.environ.get(\"OCI__SYNC_DIR\") + \"/artifacts\"\n",
    "tb_logs_path = os.path.join(artifacts_dir,\"logs\")\n",
    "check_point_path =  os.path.join(artifacts_dir,\"ckpts\",'checkpoint-{epoch}.h5')\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(check_point_path))\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(tb_logs_path))\n",
    "\n",
    "# Train the model.\n",
    "# Horovod: adjust number of steps based on number of GPUs.\n",
    "@hvd.elastic.run\n",
    "def train(state):\n",
    "    state.model.fit(dataset, steps_per_epoch=500 // hvd.size(),\n",
    "                    epochs=2-state.epoch, callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "train(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n",
    "\n",
    "### Initialize a distributed-training folder\n",
    "Next step would be to create a distributed-training workspace. Execute the following command to fetch the 'horovod-tensorflow' framework. This would create a directory 'oci_dist_training_artifacts'. The directory essentially contains artifacts(dockerfile, configurations, gloo code etc) to build a horovod job docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ads opctl distributed-training init --framework horovod-tensorflow --version v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='hvdjob-cpu-tf'\n",
    "IMAGE_TAG=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f oci_dist_training_artifacts/horovod/v1/docker/tensorflow.cpu.Dockerfile -t $IMAGE_NAME:$IMAGE_TAG ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code('train.py') is assumed to be in the current working directory. This can be overwritten using the 'CODE_DIR' build arg.\n",
    "\n",
    "`docker build --build-arg CODE_DIR=<code_folder> -f oci_dist_training_artifacts/horovod/docker/tensorflow.cpu.Dockerfile -t $IMAGE_NAME:$IMAGE_TAG .`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the Docker Image to your Tenancy OCIR\n",
    "Steps\n",
    "1. Follow the instructions to setup container registry from [here](https://docs.oracle.com/en-us/iaas/Content/Registry/Tasks/registrypushingimagesusingthedockercli.htm)\n",
    "2. Make sure you create a repository in OCIR to push the image\n",
    "3. Tag Local Docker image that needs to be pushed. \n",
    "4. Push the Docker image from your machine to OCI Container Registry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag Docker image\n",
    "Please replace the TENANCY_NAMESPACE (you can find this in tenancy information on oci console) and REGION_CODE [iad|phx ..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $IMAGE_NAME:$IMAGE_TAG iad.ocir.io/<TENANCY_NAMESPACE>/horovod:$IMAGE_NAME:$IMAGE_TAG\n",
    "\n",
    "# Example: !docker tag $IMAGE_NAME:$IMAGE_TAG iad.ocir.io/ociodscdev/horovod/$IMAGE_NAME:$IMAGE_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push <REGION_CODE>.ocir.io/<TENANCY_NAMESPACE>/horovod/$IMAGE_NAME:$IMAGE_TAG\n",
    "\n",
    "#Example: !docker push iad.ocir.io/ociodscdev/horovod/$IMAGE_NAME:$IMAGE_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define your workload yaml:\n",
    "\n",
    "The yaml file is a declarative way to express the workload.\n",
    "Create a workload yaml file called `train.yaml` to specify the run config.\n",
    "\n",
    "Workload yaml file has the following format.\n",
    "<br>\n",
    "\n",
    "```yaml\n",
    "kind: distributed\n",
    "apiVersion: v1.0\n",
    "spec:\n",
    "  infrastructure: # This section maps to Job definition. Does not include environment variables\n",
    "    kind: infrastructure\n",
    "    type: dataScienceJob\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      projectId: oci.xxxx.<project_ocid>\n",
    "      compartmentId: oci.xxxx.<compartment_ocid>\n",
    "      displayName: HVD-Distributed-TF\n",
    "      logGroupId: oci.xxxx.<log_group_ocid>\n",
    "      logId: oci.xxx.<log_ocid>\n",
    "      subnetId: oci.xxxx.<subnet-ocid>\n",
    "      shapeName: VM.Standard2.4\n",
    "      blockStorageSize: 50\n",
    "  cluster:\n",
    "    kind: HOROVOD\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      image: \"iad.ocir.io/<tenancy_id>/<repo_name>/<image_name>:<image_tag>\"\n",
    "      workDir:  \"oci://<bucket_name>@<bucket_namespace>/<bucket_prefix>\"\n",
    "      name: \"horovod_tf\"\n",
    "      config:\n",
    "        env:\n",
    "          # MIN_NP, MAX_NP and SLOTS are inferred from the shape. Modify only when needed.\n",
    "          # - name: MIN_NP\n",
    "          #   value: 2\n",
    "          # - name: MAX_NP\n",
    "          #   value: 4\n",
    "          # - name: SLOTS\n",
    "          #   value: 2\n",
    "          - name: WORKER_PORT\n",
    "            value: 12345\n",
    "          - name: START_TIMEOUT #Optional: Defaults to 600.\n",
    "            value: 600\n",
    "          - name: ENABLE_TIMELINE # Optional: Disabled by Default.Significantly increases training duration if switched on (1).\n",
    "            value: 0\n",
    "          - name: SYNC_ARTIFACTS #Mandatory: Switched on by Default.\n",
    "            value: 1\n",
    "          - name: WORKSPACE #Mandatory if SYNC_ARTIFACTS==1: Destination object bucket to sync generated artifacts to.\n",
    "            value: \"<bucket_name>\"\n",
    "          - name: WORKSPACE_PREFIX #Mandatory if SYNC_ARTIFACTS==1: Destination object bucket folder to sync generated artifacts to.\n",
    "            value: \"<bucket_prefix>\"\n",
    "          - name: HOROVOD_ARGS # Parameters for cluster tuning.\n",
    "            value: \"--verbose\"\n",
    "      main:\n",
    "        name: \"scheduler\"\n",
    "        replicas: 1 #this will be always 1\n",
    "      worker:\n",
    "        name: \"worker\"\n",
    "        replicas: 2 #number of workers\n",
    "  runtime:\n",
    "    kind: python\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      entryPoint: \"/code/train.py\" #location of user's training script in docker image.\n",
    "      args:  #any arguments that the training script requires.\n",
    "      env:\n",
    "```\n",
    "<br> <br>\n",
    "The following variables are tenancy specific that needs to be modified.\n",
    "\n",
    "| Variable | Description |\n",
    "| :-------- | :----------- |\n",
    "|compartmentId|OCID of the compartment where Data Science projects are created|\n",
    "|projectId|OCID of the project created in Data Science service|\n",
    "|subnetId|OCID of the subnet attached your Job|\n",
    "|logGroupId|OCID of the log group for JobRun logs|\n",
    "|image|Image from OCIR to be used for JobRuns|\n",
    "|workDir|URL to the working directory for opctl|\n",
    "|WORKSPACE|Workspace with the working directory to be used|\n",
    "|entryPoint|The script to be executed when launching the container|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.yaml\n",
    "\n",
    "kind: distributed\n",
    "apiVersion: v1.0\n",
    "spec:\n",
    "  infrastructure: # This section maps to Job definition. Does not include environment variables\n",
    "    kind: infrastructure\n",
    "    type: dataScienceJob\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      projectId: oci.xxxx.<project_ocid>\n",
    "      compartmentId: oci.xxxx.<compartment_ocid>\n",
    "      displayName: HVD-Distributed-TF\n",
    "      logGroupId: oci.xxxx.<log_group_ocid>\n",
    "      logId: oci.xxx.<log_ocid>\n",
    "      subnetId: oci.xxxx.<subnet-ocid>\n",
    "      shapeName: VM.Standard2.4\n",
    "      blockStorageSize: 50\n",
    "  cluster:\n",
    "    kind: HOROVOD\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      image: \"iad.ocir.io/<tenancy_id>/<repo_name>/<image_name>:<image_tag>\"\n",
    "      workDir:  \"oci://<bucket_name>@<bucket_namespace>/<bucket_prefix>\"\n",
    "      name: \"horovod_tf\"\n",
    "      config:\n",
    "        env:\n",
    "          # MIN_NP, MAX_NP and SLOTS are inferred from the shape. Modify only when needed.\n",
    "          # - name: MIN_NP\n",
    "          #   value: 2\n",
    "          # - name: MAX_NP\n",
    "          #   value: 4\n",
    "          # - name: SLOTS\n",
    "          #   value: 2\n",
    "          - name: WORKER_PORT\n",
    "            value: 12345\n",
    "          - name: START_TIMEOUT #Optional: Defaults to 600.\n",
    "            value: 600\n",
    "          - name: ENABLE_TIMELINE # Optional: Disabled by Default.Significantly increases training duration if switched on (1).\n",
    "            value: 0\n",
    "          - name: SYNC_ARTIFACTS #Mandatory: Switched on by Default.\n",
    "            value: 1\n",
    "          - name: WORKSPACE #Mandatory if SYNC_ARTIFACTS==1: Destination object bucket to sync generated artifacts to.\n",
    "            value: \"<bucket_name>\"\n",
    "          - name: WORKSPACE_PREFIX #Mandatory if SYNC_ARTIFACTS==1: Destination object bucket folder to sync generated artifacts to.\n",
    "            value: \"<bucket_prefix>\"\n",
    "          - name: HOROVOD_ARGS # Parameters for cluster tuning.\n",
    "            value: \"--verbose\"\n",
    "      main:\n",
    "        name: \"scheduler\"\n",
    "        replicas: 1 #this will be always 1\n",
    "      worker:\n",
    "        name: \"worker\"\n",
    "        replicas: 2 #number of workers\n",
    "  runtime:\n",
    "    kind: python\n",
    "    apiVersion: v1.0\n",
    "    spec:\n",
    "      entryPoint: \"/code/train.py\" #location of user's training script in docker image.\n",
    "      args:  #any arguments that the training script requires.\n",
    "      env:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ads opctl to create the cluster infrastructure and run the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dry Run To check the runtime configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ads opctl run -f train.yaml --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ads opctl run -f train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would emit the following information about the job created and the job runs within.<br>\n",
    "`jobId: <job_id>`<br>\n",
    "`mainJobRunId: <scheduer_job_run_id>`<br>\n",
    "`workDir: oci://<bucket_name>@<bucket_namespace>/<bucket_prefix>`<br>\n",
    "`workerJobRunIds:`<br>\n",
    "`- <worker_1_jobrun_id>`<br>\n",
    "`- <worker_2_jobrun_id>`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor logs\n",
    "You can monitor the logs emitted from the job runs using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ads jobs watch <jobrun_id>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notice": "Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
